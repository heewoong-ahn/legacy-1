import { DataSource } from 'typeorm';
import { LlmModel } from '../entities/llm-model.entity';
import { LlmTest } from '../entities/llm-test.entity';
import * as dotenv from 'dotenv';

dotenv.config();

const dataSource = new DataSource({
  type: 'postgres',
  host: process.env.DB_HOST || 'localhost',
  port: parseInt(process.env.DB_PORT || '5432'),
  username: process.env.DB_USERNAME || 'postgres',
  password: process.env.DB_PASSWORD || 'password',
  database: process.env.DB_NAME || 'legacy_1',
  entities: [LlmModel, LlmTest],
  synchronize: true,
});

async function createLlmModels() {
  try {
    console.log('ğŸ”— ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì¤‘...');
    await dataSource.initialize();
    console.log('âœ… ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„±ê³µ');
    
    const llmModelRepository = dataSource.getRepository(LlmModel);
    
    // ê¸°ì¡´ ëª¨ë¸ í™•ì¸
    const existingModels = await llmModelRepository.find();
    if (existingModels.length > 0) {
      console.log('âš ï¸  LLM ëª¨ë¸ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.');
      console.log(`í˜„ì¬ ë“±ë¡ëœ ëª¨ë¸ ìˆ˜: ${existingModels.length}`);
      existingModels.forEach(model => {
        console.log(`- ${model.name} (${model.provider})`);
      });
      return;
    }
    
    console.log('ğŸš€ LLM ëª¨ë¸ ìƒì„± ì‹œì‘...');
    
    // ê¸°ë³¸ LLM ëª¨ë¸ë“¤ ìƒì„±
    const models = [
      {
        name: 'GPT-4',
        provider: 'OpenAI',
        description: 'OpenAIì˜ ê³ ì„±ëŠ¥ ì–¸ì–´ ëª¨ë¸ë¡œ, ë³µì¡í•œ ì¶”ë¡ ê³¼ ì°½ì‘ ì‘ì—…ì— ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.',
        is_active: true,
        config: {
          max_tokens: 4096,
          temperature: 0.7,
          top_p: 1,
          frequency_penalty: 0,
          presence_penalty: 0,
        },
      },
      {
        name: 'GPT-3.5-turbo',
        provider: 'OpenAI',
        description: 'OpenAIì˜ ë¹ ë¥´ê³  íš¨ìœ¨ì ì¸ ì–¸ì–´ ëª¨ë¸ë¡œ, ì¼ë°˜ì ì¸ ëŒ€í™”ì™€ í…ìŠ¤íŠ¸ ì‘ì—…ì— ì í•©í•©ë‹ˆë‹¤.',
        is_active: true,
        config: {
          max_tokens: 4096,
          temperature: 0.7,
          top_p: 1,
          frequency_penalty: 0,
          presence_penalty: 0,
        },
      },
      {
        name: 'Claude-3',
        provider: 'Anthropic',
        description: 'Anthropicì˜ ì•ˆì „í•˜ê³  ìœ ìš©í•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ë¡œ, ê¸´ ë¬¸ë§¥ ì´í•´ì— ê°•ì ì„ ê°€ì§‘ë‹ˆë‹¤.',
        is_active: true,
        config: {
          max_tokens: 4096,
          temperature: 0.7,
          top_p: 1,
        },
      },
      {
        name: 'Gemini-Pro',
        provider: 'Google',
        description: 'Googleì˜ ë©€í‹°ëª¨ë‹¬ AI ëª¨ë¸ë¡œ, í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ë¥¼ í•¨ê»˜ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.',
        is_active: true,
        config: {
          max_tokens: 4096,
          temperature: 0.7,
          top_p: 1,
        },
      },
    ];
    
    for (const modelData of models) {
      const model = llmModelRepository.create(modelData);
      await llmModelRepository.save(model);
      console.log(`âœ… LLM ëª¨ë¸ ìƒì„±ë¨: ${model.name} (${model.provider})`);
    }
    
    console.log('ğŸ‰ ëª¨ë“  LLM ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.');
    console.log(`ì´ ${models.length}ê°œì˜ ëª¨ë¸ì´ ë“±ë¡ë˜ì—ˆìŠµë‹ˆë‹¤.`);
    
  } catch (error) {
    console.error('âŒ LLM ëª¨ë¸ ìƒì„± ì¤‘ ì˜¤ë¥˜:', error);
    if (error.code === 'ECONNREFUSED') {
      console.error('ğŸ’¡ PostgreSQL ì„œë²„ê°€ ì‹¤í–‰ë˜ê³  ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.');
    } else if (error.code === '3D000') {
      console.error('ğŸ’¡ ë°ì´í„°ë² ì´ìŠ¤ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ë¨¼ì € ìƒì„±í•´ì£¼ì„¸ìš”.');
    } else if (error.code === '28P01') {
      console.error('ğŸ’¡ ë°ì´í„°ë² ì´ìŠ¤ ì¸ì¦ ì •ë³´ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.');
    }
    process.exit(1);
  } finally {
    if (dataSource.isInitialized) {
      await dataSource.destroy();
      console.log('ğŸ”Œ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì¢…ë£Œ');
    }
  }
}

// ìŠ¤í¬ë¦½íŠ¸ê°€ ì§ì ‘ ì‹¤í–‰ë  ë•Œë§Œ í•¨ìˆ˜ í˜¸ì¶œ
if (require.main === module) {
  createLlmModels();
}